---
title: "Forecasting Flood Inundation"
author: "Kristin Chang & Jenna Epstein"
date: "March 31, 2021"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
```

# Motivation
According to the World Health Organization, floods are the most frequent natural disaster affecting countries across the globe. Floods have the potential to leave communities devastated not only because of the direct loss of life due to drowning and infrastructure damage, but also because of indirect impacts such as increased transmission of disease, higher risk of injury and hypothermia, disrupted or disabled infrastructure systems, and increased likelihood of causing other natural disasters. In the past ten years, climate change has exacerbated the effects of natural disasters like flooding, drought, sea level rise, and extreme precipitation and their frequency and intensity are expected to continue to rise if left unchecked.

City planners have the utmost responsibility to ensure their cities are prepared to respond to such natural disasters and minimize harm to their most vulnerable communities. The following document outlines a machine learning algorithm that predicts which areas of a city are at the highest risk of flooding disasters. An Emergency Management team can use the resulting model of this algorithm to inform their preparedness tactics to minimize damage and respond to future flooding disasters more efficiently. Ultimately, the algorithm is a helpful tool for individuals in Public Works, Public Health, City Planning, and Community-Based Organizations to proactively plan for resilience. 

The model is created using past flood data from the City of **Calgary** in Canada to measure accuracy and then is applied to the City of **Denver** in the United States to measure generalizability. This document explains each step of the process such that City Planning departments can input their city’s data and accurately interpret the resulting model. We also wanted to provide transparency into the feature engineering steps of the process, which is why the procedures in ArcGIS and R are described in detail.

For audiences who would like the main takeaways, we invite you to view [this three-page summary document.](https://github.com/kristinchang/CPLN675_midterm/blob/main/LUEPmidterm_summary_ChangEpstein.pdf)


# Setup
The algorithm was created using ArcGIS Pro and R. First, the necessary R libraries are loaded and themes are defined for the maps and plots that will be displayed later in the process.

```{r load libraries and functions, message=FALSE, warning=FALSE}
# SETUP

# load libraries
library(caret)
library(pscl)
library(plotROC)
library(pROC)
library(sf)
library(tidyverse)
library(knitr)
library(kableExtra)
library(FNN)
library(scales)
library(jtools)
library(viridis)
library(gridExtra)


# load themes and palettes
mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.text.x = element_text(size = 14))
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}

```

# Calgary Data

## Initial Data
The fishnet was generated in ArcGIS Pro using the **Create Fishnet** tool in order to generate a grid of the appropriate extent with 200 meter by 200 meter cells as the unit of measurement. This cell size was determined using the overall extent of the city boundary and the size of the resulting dataset. Cell size will vary depending on the city of focus and the computing power of the machine being used. 

The inundation raster data was also generated in ArcGIS Pro using the **Reclassify** tool to indicate pixels within the flooding extent and pixels outside of the flooding extent. Inundated pixels are assigned a value of 1 and not inundated pixels are assigned a value of 0. **Zonal Statistics** were then used to calculate the maximum level of flooding per fishnet grid cell in order to visualize and identify which grid cells are at the highest risk of flooding. 

```{r eval=FALSE, include=FALSE}
#code used to generate introductory map, for reference.
#ggplot() + 
#  geom_sf(data=calgary_fishnet_final, aes(fill=as.factor(inundation)), color=NA) +
#  scale_fill_manual(values = c("navy", "darkgreen"),
#                    labels = c("yes","no"),
#                    name = "Inundation: \nDid Flooding Occur?") +
#  labs(title="Inundation in Calgary", subtitle = "Extent derived from raster from June 2013 flooding") +
# # geom_sf(data=calgary_boundary, color="black", fill=NA)+
#  mapTheme()

```

```{r probcell, echo=FALSE, out.width="60%", fig.show='hold', fig.align='center'}
knitr::include_graphics("C:/Users/jenna/Documents/GitHub/CPLN675_midterm/calgary_inundation_intromap.png")
```

```{r initial fishnet and boundary, message=FALSE, warning=FALSE, results='hide'}
# CALGARY: Load Calgary fishnet generated in ArcGIS Pro (200x200m cells).
calgary_fishnet <- st_read("Data_Calgary/Calgary_new/for_R/calgary_fishnet.shp") %>%
st_as_sf() %>% st_transform('EPSG:3780') %>% rename(ID_FISHNET = ID_fishnet)

calgary_fishnet$ID_FISHNET <-as.character(calgary_fishnet$ID_FISHNET)

calgary_boundary <- st_read("provided_midTermProject_Data/CALGIS_CITYBOUND_LIMIT/CALGIS_CITYBOUND_LIMIT.shp")
calgary_boundary <- calgary_boundary %>% st_transform('EPSG:3780')

#ggplot() +
#  geom_sf(data=calgary_boundary, fill="grey") +
#  geom_sf(data=calgary_fishnet, alpha=0.2) + mapTheme()

```

## Feature Engineering
Next, data for features that contribute to flooding and flood risk is loaded into ArcGIS Pro for calculations. The following sections describe this process in brief and explains why they are included in the model or not. Once the calculations are complete, the resulting data tables are exported from ArcGIS Pro and loaded into R to inform the model. We also used a select-by-centroids approach to select fishnet grid cells that have centroids situated within the 2013 flood extent.

Knowledge about the characteristics of the land such as topography, land cover, and hydrology can indicate how susceptible the area is to floods. 

```{r all code for feature engineering to run before creating maps, message=FALSE, warning=FALSE, results='hide'}
# CALGARY: Inundation classification (generated by reclassifying the initial inundation raster, and calculated using zonal statistics (maximum)).
calgary_inundation <- st_read("Data_Calgary/Calgary_new/for_R/cal_inun_max.xls")  %>%
  dplyr::select(ID_FISHNET, MAX) %>%
  #dplyr::rename(ID_fishnet = FID) %>%
  dplyr::rename(inundation = MAX)

calgary_inundation$ID_FISHNET <-as.character(calgary_inundation$ID_FISHNET)

calgary_inundation_grid <- left_join(calgary_inundation, calgary_fishnet)
calgary_fishnet_new <-  calgary_inundation_grid

# CALGARY: Elevation mean (calculated using zonal statistics (mean)).
calgary_elev <- st_read("Data_Calgary/Calgary_new/for_R/cal_elev_mean.xls") %>%
  dplyr::select(ID_FISHNET, MEAN) %>%
  dplyr::rename(elevation_mean = MEAN)

calgary_elev$ID_FISHNET <-as.character(calgary_elev$ID_FISHNET)

calgary_elev_grid <- full_join(calgary_elev, calgary_fishnet_new) %>% st_as_sf()
calgary_fishnet_new <- calgary_elev_grid

# CALGARY: Stream network, generated using the ArcHydro workflow from the week4 lab in ArcGIS Pro.
calgary_streams <- st_read("Data_Calgary/Calgary_new/for_R/Hydrology/cal_stream.shp") 
calgary_streams_neardist <- st_read("Data_Calgary/Calgary_new/for_R/cal_stream_neardist.xls")  %>%
  dplyr::select(-Rowid, -OBJECTID, -NEAR_FID) %>%
  dplyr::rename(ID_FISHNET = IN_FID) %>%
  dplyr::rename(dist_stream = NEAR_DIST)

## used the generate near table in ArcGIS to calculate from fishnet to stream
calgary_streams_neardist$ID_FISHNET <-as.character(calgary_streams_neardist$ID_FISHNET)

calgary_streamneardist_grid <- full_join(calgary_streams_neardist, calgary_fishnet_new) %>% st_as_sf()
calgary_fishnet_new <-  calgary_streamneardist_grid

# CALGARY: flow accumulation max per fishnet cell. Generated using zonal statistics (maximum).
calgary_flowacc <- st_read("Data_Calgary/Calgary_new/for_R/cal_flowacc_max.xls") %>%
  dplyr::rename(flowacc_max = MAX) %>%
  dplyr::select(-Rowid, -COUNT, -AREA, -ZONE.CODE)

calgary_flowacc$ID_FISHNET <-as.character(calgary_flowacc$ID_FISHNET)

calgary_flowacc_grid <- full_join(calgary_flowacc, calgary_fishnet_new) %>%st_as_sf()
calgary_fishnet_new <- calgary_flowacc_grid 


# CALGARY: Parks distance - from open data calgary. Used arcgis pro to create a near distance table.
calgary_parks_neardist <- st_read("Data_Calgary/Calgary_new/for_R/cal_parksfeat_near.xls")  %>%
  dplyr::select(-Rowid, -OBJECTID, -NEAR_FID) %>%
  dplyr::rename(ID_FISHNET = IN_FID) %>%
  dplyr::rename(dist_parks = NEAR_DIST)

calgary_parks_neardist$ID_FISHNET <-as.character(calgary_parks_neardist$ID_FISHNET)

calgary_parks_neardist_grid <- full_join(calgary_parks_neardist, calgary_fishnet_new) %>% st_as_sf()
calgary_fishnet_new <-  calgary_parks_neardist_grid


# CALGARY: Slope max per fishnet cell. Generated using zonal statistics (max). Ended up not using this in the model, but still explored in feature engineering.

calgary_slope <- st_read("Data_Calgary/Calgary_new/for_R/cal_slope_max.xls") %>%
  dplyr::rename(slope_max = MAX) %>%
  dplyr::select(-Rowid, -COUNT, -AREA, -ZONE.CODE)

calgary_slope$ID_FISHNET <-as.character(calgary_slope$ID_FISHNET)

calgary_slope_grid <- full_join(calgary_slope, calgary_fishnet_new) %>% st_as_sf()
calgary_fishnet_new <- calgary_slope_grid

# CALGARY: Distance to steep slopes per fishnet cell. Generated using near table, near distance.
calgary_steepslope_dist <- st_read("Data_Calgary/Calgary_new/for_R/cal_steepslope_dist.xls")  %>%
  dplyr::select(-Rowid, -OBJECTID, -NEAR_FID) %>%
  dplyr::rename(ID_FISHNET = IN_FID) %>%
  dplyr::rename(steepslope_dist = NEAR_DIST)

calgary_steepslope_dist$ID_FISHNET <-as.character(calgary_steepslope_dist$ID_FISHNET)

calgary_steepslope_dist_grid <- full_join(calgary_steepslope_dist, calgary_fishnet_new) %>%st_as_sf()
calgary_fishnet_new <- calgary_steepslope_dist_grid 

# CALGARY: Intersections with hydro features
#shapefile import
calgary_hydro <- st_read("Data_Calgary/Calgary_new/for_R/Hydrology/hydrology_features.shp") %>%
  st_transform(st_crs(calgary_fishnet_new))

#visualize hydrology features - more extensive than what the stream network from ArcGIS hydrology workflow. so it is streams + major hydrology features like lakes and ponds, etc.
#ggplot() +
#  geom_sf(data=calgary_boundary, fill="grey") +
#  #geom_sf(data=calgary_fishnet, alpha=0.2) + 
#  geom_sf(data=calgary_hydro, color="blue") +mapTheme()

calgary_fishnet_new <- calgary_fishnet_new %>% mutate(n_hydro_int = lengths(st_intersects(calgary_fishnet_new, calgary_hydro)))

# CALGARY: majority land cover type per fishnet cell. Reclassified the data by land cover impervious and not impervious, converted it to a raster, and used zonal statistics (sum) for pixels. Not used in the model ultimately, but still explored in feature engineering process - worthwhile to discuss.

calgary_impervious_sum <- st_read("Data_Calgary/Calgary_new/for_R/cal_lcimp_sumt.xls") %>%
  dplyr::rename(imperv_pixels_sum = SUM) %>%
  dplyr::select(-Rowid, -COUNT, -AREA, -ZONE.CODE)

calgary_impervious_sum$ID_FISHNET <-as.character(calgary_impervious_sum$ID_FISHNET)
calgary_impervious_sum_grid <- full_join(calgary_impervious_sum, calgary_fishnet_new) %>% st_as_sf()
calgary_impervious_sum_grid[is.na(calgary_impervious_sum_grid)] = 0
calgary_fishnet_new <- calgary_impervious_sum_grid

```

```{r calgary select fishnet cells by centroid, message=FALSE, warning=FALSE, results='hide' }
# Centroid-in-polygon join to see which cells have their centroid in the calgary boundary
calgary_selectCentroids <-
  st_centroid(calgary_fishnet_new)[calgary_boundary,] %>%
  st_drop_geometry() %>%
  left_join(dplyr::select(calgary_fishnet_new, ID_FISHNET)) %>%
  st_sf()

calgary_fishnet_final <- calgary_selectCentroids

```

```{r calgary inundation by way of fishnet, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, results='hide'}
ggplot() + 
  geom_sf(data=calgary_fishnet_final, aes(fill=as.factor(inundation)), color=NA) +
  scale_fill_manual(values = c("darkgreen", "navy"),
                    labels = c("no","yes"),
                    name = "Did Flooding Occur?") +
  labs(title="Inundation in Calgary", subtitle = "Extent derived from raster from June 2013 flooding") + theme(legend.position = "bottom") +
 # geom_sf(data=calgary_boundary, color="black", fill=NA)+
  mapTheme()

```

```{r, convert variables all to numeric, message=FALSE, warning=FALSE, results='hide'}
calgary_fishnet_final <- calgary_fishnet_final %>%
  dplyr::select(-Shape_Leng, -Shape_Area)

calgary_fishnet_final$inundation <-as.numeric(as.character(calgary_fishnet_final$inundation))
calgary_fishnet_final$slope_max <-as.numeric(as.character(calgary_fishnet_final$slope_max))
calgary_fishnet_final$flowacc_max <-as.numeric(as.character(calgary_fishnet_final$flowacc_max))
calgary_fishnet_final$elevation_mean <-as.numeric(as.character(calgary_fishnet_final$elevation_mean))
calgary_fishnet_final$imperv_pixels_sum <-as.numeric(as.character(calgary_fishnet_final$imperv_pixels_sum))
calgary_fishnet_final$dist_stream <-as.numeric(as.character(calgary_fishnet_final$dist_stream))
calgary_fishnet_final$n_hydro_int <-as.numeric(as.character(calgary_fishnet_final$n_hydro_int))
calgary_fishnet_final$dist_parks <-as.numeric(as.character(calgary_fishnet_final$dist_parks))
calgary_fishnet_final$steepslope_dist <-as.numeric(as.character(calgary_fishnet_final$steepslope_dist))
```

## Significant Features

### Mean Elevation
Elevation generally describes the topography of the land. Areas at lower levels of elevation are at higher risk for floods than areas at higher elevations. We explored calculating the lowest degree of elevation per fishnet grid cell in Calgary, but found that using the mean helped the performance of our model. Mean elevation is stored in the **`elevation_mean`** feature variable, measured in meters.

To calculate our **`elevation_mean`** feature variable, a Digital Elevation Model (DEM) was loaded into ArcGIS. **Zonal Statistics** were used to calculate the mean per fishnet grid cell.

```{r message=FALSE, warning=FALSE, results='hide'}
map.elevation <- ggplot() +
  geom_sf(data = calgary_fishnet_final, aes(fill = elevation_mean), color=NA) +
  scale_fill_viridis()+
  labs(title = "Elevation \n(Mean)", fill="Elevation \n(meters)") + mapTheme()

```


### Distance to Nearest Stream
Understanding the structure of streams helps inform the intensity of potential flooding. If the water level of the stream exceeds the stream channel, then flooding occurs. Areas that are closer to streams are more vulnerable to floods than areas that are farther away.

The hydrology tools in ArcGIS Pro are used to turn the Calgary DEM into a stream network. First, the **Fill** tool was used to fill any sinks. The resulting surface is input in the **Flow Direction** tool to generate a raster showing the direction of flow out of each pixel. Direction is used to calculate **Flow Accumulation** per pixel - this step is described further in the next feature section. Assuming a drainage threshold of 25 square kilometers, **Set Null** is used to calculate the number of pixels that represent a drainage of this area or more, assign these cells a value of 1 and all other pixels a value of NODATA. The resulting Calgary stream network is displayed below. Lastly, the **Generate Near Table** tool is used to calculate the distance of each fishnet grid cell to a stream, stored as the `dist_stream` feature variable, measured in meters.

```{r cal streams map, message=FALSE, warning=FALSE, results='hide'}
#visualize streams
map.streams <- ggplot() +
  geom_sf(data=calgary_boundary, fill="grey") +
  #geom_sf(data=calgary_fishnet, alpha=0.2) + 
  geom_sf(data=calgary_streams, color="blue") + labs(title="Stream Network in Calgary") + mapTheme()

#distance to streams
map.diststreams <- ggplot() +
  geom_sf(data = calgary_fishnet_final, aes(fill = dist_stream), color=NA) +
  scale_fill_viridis()+
  labs(title = "Distance to Nearest \nStream", fill="Distance \n(meters)") + mapTheme()
```

### Maximum Flow Accumulation
Flooding occurs when water levels exceed the stream channel. Based on the direction of the streams calculated above, **Flow Accumulation** tool is used to calculate the accumulated weight of all pixels flowing into each downslope pixel in the raster. **Zonal Statistics** (maximum) is used to calculate the maximum flow accumulation per each fishnet grid cell. Using the maximum value allows us to evaluate at the highest risk level. In other words, the resulting model will present a “worst-case-scenario” picture enabling planners to be over-prepared rather than under-prepared. This feature variable is called **`flow_maxacc`**. 

```{r message=FALSE, warning=FALSE, results='hide'}
map.flowaccmax <- ggplot() +
  geom_sf(data = calgary_fishnet_final, aes(fill = flowacc_max), color=NA) +
  scale_fill_viridis(breaks = c(400000, 800000, 1200000), labels = c("400K", "800K", "1200K"))+
  labs(title = "Flow Accumulation \n(Maximum)", fill="Pixels \n") + mapTheme()

```

### Distance to Nearest Steep Slope
Steep slopes in the land can contribute to both flow direction and flow accumulation. Additionally, a steeper slope likely accelerates the speed of flow and can intensify in the case of a flood. Thus, it is important to be aware of the location of steep slopes in the topography and their relative distance to past floods. 

Slopes can be calculated using the **Slope** tool in ArcGIS. To distinguish which slopes are steep, **Reclassify** is used to assign slopes greater than or equal to 10 a value of 1 and all other slopes a value of NODATA. These steep slopes are converted from raster data to polygon features using the **Raster to Polygon** tool. Then, **Near** can be used once again to calculate the distance of each fishnet grid cell to a steep slope. this is imported into R as the **`dist_steepslope`** feature variable.

```{r message=FALSE, warning=FALSE, results='hide'}
map.steepslopedist <- ggplot() +
  geom_sf(data = calgary_fishnet_final, aes(fill = steepslope_dist), color=NA) +
  scale_fill_viridis()+
  labs(title = "Distance to \nNearest Steep Slope", fill="Distance \n(meters)") + mapTheme()
```

### Distance to Nearest Park
Land cover or soil type can be indicative of areas prone to flooding. The more permeable the surface is, the more risky it is for floods. Initially, land cover data for Calgary was used to identify which fishnet grid cells were impervious based on their land cover type; however, this did not help the model perform well. Instead, distance to parks is incorporated into the model. Parks are generally covered in grass which is fairly permeable. Thus, the closer a fishnet grid cell is to a park, the more likely it is to become inundated. 

After bringing in the complete parks dataset in Calgary from the city's open data portal, the **Near** tool in ArcGIS is used to perform this calculation for the **`distance to parks`** feature variable. 
```{r message=FALSE, warning=FALSE, results='hide'}
#distance to parks
map.distparks <- ggplot() +
  geom_sf(data = calgary_fishnet_final, aes(fill = dist_parks), color=NA) +
  scale_fill_viridis()+
  labs(title = "Distance to \nNearest Park", fill="Distance \n(meters)") + mapTheme()
```

### Number of Hydrological Feature Intersections
In addition to mere distance to stream lines, the frequency of intersections with hydrological features could indicate damper soils and areas at greater risk of inundation. To calculate the number of intersections with existing hydrological features per fishnet grid cell, the `lengths(st_intersects)` functionality in R is used to create a count feature variable, **`n_hydro_int`**.

```{r message=FALSE, warning=FALSE, results='hide'}
#number of hydrology feature intersections
map.hydroints <- ggplot() +
  geom_sf(data = calgary_fishnet_final, aes(fill = n_hydro_int), color=NA) +
  scale_fill_viridis()+
  labs(title = "Number of \nHydrology Intersections", fill="Number \n") + mapTheme()
```
### Maps of All Significant Features included in the Model
```{r, message=FALSE, warning=FALSE}

grid.arrange(map.elevation, map.diststreams, map.flowaccmax, map.steepslopedist, map.distparks, map.hydroints,
             ncol=3,
             top = "")
```


```{r eval=FALSE, include=FALSE}
#writing what I have so far to a .csv also in github, so you can also just load this back in and do a join to the fishnet shapefile (using ID_fishnet as the joiner)
#st_write(calgary_fishnet_final, "calgary_fishnet_final_new.csv")

```

## Plotting Features per Inundation Outcome
These six selected significant features are shown below in the plots to show differences in across fishnet grid cells that flooded and those that did not, according to the classification from the 2013 inundation extent. 

```{r message=FALSE, warning=FALSE, results='hide'}
inundationPlotVariables <- 
  calgary_fishnet_final %>% st_drop_geometry()

# plotting independent variables for elevation_mean and slope_max
calgary_fishnet_variables <- inundationPlotVariables %>%
  dplyr::select(inundation, elevation_mean, steepslope_dist, dist_stream, flowacc_max, n_hydro_int, dist_parks) %>%
  gather(key, value, elevation_mean:dist_parks)%>%
   mutate(value = ifelse(key == "flowacc_max", value/10, value)) %>%
  mutate(value=ifelse(key=="n_hydro_int", value*10000, value))


ggplot(calgary_fishnet_variables, aes(as.factor(inundation), value, fill=as.factor(inundation))) + 
    geom_bar(stat = "identity")+
    facet_wrap(~key) +
    scale_y_continuous(labels = comma) +
  scale_fill_manual(values = c("darkgreen", "navy"),
                    labels = c("Not Flooded","Flooded"),
                    name = "")   + labs(x="Inundation: Flooding or Not?", y="Value", title= "Independent Variables used in the Model", caption="\nNumber of hydro intersections scaled up by 10,000 \nFlow Accumulation (max) scaled down by 10") + plotTheme()
```

```{r eval=FALSE, include=FALSE}
#plotting these again, but this time using stat="summary" and mean for the function. also, scaling slope to see if there is a discernable difference. 
calgary_fishnet_variables <- inundationPlotVariables %>%
  dplyr::select(inundation, elevation_mean, steepslope_dist, dist_stream, dist_parks) %>%
  gather(key, value, elevation_mean:dist_parks)
#%>%  mutate(value = ifelse(key == "slope_max", value*100, value))


ggplot(calgary_fishnet_variables, aes(as.factor(inundation), value, fill=as.factor(inundation))) + 
    geom_bar(stat = "summary", fun="mean") +
    facet_wrap(~key) +
    scale_y_continuous(labels = comma) +
  scale_fill_manual(values = c("dodgerblue4", "darkgreen"),
                    labels = c("Not Inundated","Inundated"),
                    name = "")   + labs(x="Inundation", y="Mean") +plotTheme()
```
```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, results='hide'}
# flow accumulation max, visualizing separately
calgary_fishnet_variable_imperv <- inundationPlotVariables %>%
  dplyr::select(inundation, flowacc_max) %>%
  gather(key, value, flowacc_max)


ggplot(calgary_fishnet_variable_imperv, aes(as.factor(inundation), value, fill=as.factor(inundation))) + 
    geom_bar(stat = "identity")+
    facet_wrap(~key) +
      scale_y_continuous(labels = comma) +
  scale_fill_manual(values = c("navy", "darkgreen"),
                    labels = c("Not Inundated","Inundated"),
                    name = "")   + labs(x="Inundation", y="Value") +plotTheme()
```
```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, results='hide'}
#imperv surface was warping the scaling, so visualizing separately
calgary_fishnet_variable_imperv <- inundationPlotVariables %>%
  dplyr::select(inundation, imperv_pixels_sum) %>%
  gather(key, value, imperv_pixels_sum)


ggplot(calgary_fishnet_variable_imperv, aes(as.factor(inundation), value, fill=as.factor(inundation))) + 
    geom_bar(stat = "identity")+
    facet_wrap(~key) +
      scale_y_continuous(labels = comma) +
  scale_fill_manual(values = c("navy", "darkgreen"),
                    labels = c("Not Inundated","Inundated"),
                    name = "")   + labs(x="Inundation", y="Value") +plotTheme()
```
```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, results='hide'}
# number of hydro intersections, visualizing separately
calgary_fishnet_variable_nhydroint <- inundationPlotVariables %>%
  dplyr::select(inundation, n_hydro_int) %>%
  gather(key, value, n_hydro_int)


ggplot(calgary_fishnet_variable_nhydroint, aes(as.factor(inundation), value, fill=as.factor(inundation))) + 
    geom_bar(stat = "identity")+
    facet_wrap(~key) +
      scale_y_continuous(labels = comma) +
  scale_fill_manual(values = c("dodgerblue4", "darkgreen"),
                    labels = c("Not Inundated","Inundated"),
                    name = "")   + labs(x="Inundation", y="Value") +plotTheme()
```

# Model Development & Testing on Calgary
Now that the features have been prepared, we can begin to build the predictive model. First, we split up our known data set into a training and a test set. 

## Iterative Development Process
A linear regression is performed on the training set to determine the correlation between the set of selected features for the training data and the inundation data for the training data. This process was repeated multiple times to determine which combination of engineered features performed the best. For instance, recall from the Distance to Parks section that an impervious surfaces feature had been engineered but ultimately was not included in the final model. Reviewing the results from multiple regressions helped us determine which combination of features would provide the best results. The results of the final model are displayed and discussed below.

```{r message=FALSE, warning=FALSE, results='hide'}
set.seed(3456)
trainIndex <- createDataPartition(calgary_fishnet_final$inundation, p = .70,
                                  list = FALSE,
                                  times = 1)
calgaryTrain <- calgary_fishnet_final[ trainIndex,]
calgaryTest  <- calgary_fishnet_final[-trainIndex,]
```

```{r message=FALSE, warning=FALSE, results='hide'}
inundationModel <- glm(inundation ~ ., family="binomial"(link="logit"),
                  data=(calgaryTrain) %>% as.data.frame() %>% dplyr::select(-ID_FISHNET, -geometry, -slope_max, -imperv_pixels_sum))
 print(inundationModel)
```

## Results
### Model Summary
The output displays various statistics that indicate the quality of the model. For instance, the p-value listed for each feature describes how significant the feature is to the outcome of whether or not a flood occurs. All of these features are statistically significant as their p-values are <0.05. Additionally, measures like the Pseudo-R^2 and the AIC score describe how closely related our features are to the outcome. This is the point at which features can be removed, added, or adjusted within the model to enhance performance.

```{r calgaryModel summ, message=FALSE, warning=FALSE}
summ(inundationModel)

```
### Histogram of classProbs
Once the features have been finalized, we can run the regression model on our test data to further examine its accuracy. The results are displayed in the histogram below showing the frequency of each probability level that a flood will occur. 


```{r create classProbs and classProbs histogram, message=FALSE, warning=FALSE, results='hide'}
classProbs <- predict(inundationModel, calgaryTest, type="response")

hist((classProbs), main = paste("Histogram of classProbs"), col = "blue", xlab = "Inundation Probability") + plotTheme()
```

### Distribution of Probabilities Visualization
The visualization below shows the distribution of outcomes for the model. It is clear that the model is better at predicting 0s (no inundation) than predicting 1s (inundation).

```{r message=FALSE, warning=FALSE}
testProbs <- data.frame(obs = as.numeric(calgaryTest$inundation),
                        pred = classProbs,
                        ID_FISHNET = calgaryTest$ID_FISHNET)

ggplot(testProbs, aes(x = pred, fill=as.factor(obs))) + geom_density() +
  facet_grid(obs ~ .) + xlab("Probability") + geom_vline(xintercept = .38) +
  scale_fill_manual(values = c("darkgreen", "navy"),
                      labels = c("Not Flooded","Flooded"),
                                 name="") +
                      labs(title = "Distribution of Probabilities") + plotTheme()
```

## Finding the Optimal Threshold
In order to best optimize this model for use, identifying the threshold that provides the greatest accuracy is important. This is done by running the `iterateThresholds` function and looking at the results for all thresholds the table of thresholds 0.01 through 0.99. The optimal threshold in this case is **0.38**.

```{r iterate thresh function, message=FALSE, warning=FALSE, results='hide'}
# iterateThresholds function
iterateThresholds <- function(data, group) {
   group <- enquo(group)
  x = .01
  all_prediction <- data.frame()
  while (x <= 1) {

  this_prediction <-
      testProbs %>%
      mutate(predOutcome = ifelse(pred > x, 1, 0)) %>%
         group_by(!!group) %>%
      dplyr::count(predOutcome, obs) %>%
      dplyr::summarize(sum_TN = sum(n[predOutcome==0 & obs==0]),
                sum_TP = sum(n[predOutcome==1 & obs==1]),
                sum_FN = sum(n[predOutcome==0 & obs==1]),
                sum_FP = sum(n[predOutcome==1 & obs==0]),
            total=sum(n)) %>%
    mutate(True_Positive = sum_TP / total,
         True_Negative = sum_TN / total,
         False_Negative = sum_FN / total,
         False_Positive = sum_FP / total,
         Accuracy = (sum_TP + sum_TN) / total, Threshold = x)

  all_prediction <- rbind(all_prediction, this_prediction)
  x <- x + .01
  }
return(all_prediction)
}
```

```{r message=FALSE, warning=FALSE}
whichThreshold <- iterateThresholds(testProbs)

allThresholds<-kable(whichThreshold) %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE)%>%
  scroll_box(width = "100%", height = "500px")

allThresholds
```


```{r message=FALSE, warning=FALSE}
testProbs$predClass  = ifelse(testProbs$pred > .38 ,1,0)

xtab.regCalgary <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")
```

## Confusion Matrix
There are four possible scenarios that will result from the use of this model:
1. Model predicts inundation and there is inundation (True Positive)
2. Model predicts no inundation and there is no inundation (True Negative)
3. Model predicts inundation and there is no inundation (False Positive)
4. Model predicts no inundation and there is inundation (False Negative)

The results of each scenario at the optimal threshold for accuracy (0.38) is displayed in the table below (with the confusion matrix of 0 and 1 directly below the table). These results support our above observation that the model predicts well for areas of no inundation than areas of inundation. 

Type of Prediction | Description | Results
------------- | ------------- | ------------- 
True Positive (TP) | Model predicts inundation and there is inundation | 425
True Negative (TN) | Model predicts no inundation and there is no inundation | 4873
False Positive (FP)| Model predicts inundation and there is no inundation | 167
False Negative (FN) | Model predicts no inundation and there is inundation |96

 


```{r message=FALSE, warning=FALSE}
as.matrix(xtab.regCalgary) %>% kable(caption = "Confusion Matrix") %>% kable_styling("striped", full_width = T, font_size = 14, position = "left")
```
## Confusion Matrix - Statistics
Further statistics associated with the confusion matrix are displayed in the table below. Of particular note are the sensitivity (rate of true positives) and specificity (rate of true negatives). While the results indicate that the model performs very well for true negatives (a specificity of 98%), it also predicts the true positives fairly well (a sensitivity of ~72%).

Since it is better to be overprepared for a flooding event than underprepared, we concluded that the number of false positive predictions was not a concern, especially given the precarious and unexpected nature of extreme weather events during an era of climate change.
```{r message=FALSE, warning=FALSE}
as.matrix(xtab.regCalgary, what="classes") %>% kable(caption = "Confusion Matrix - Statistics") %>% kable_styling(font_size = 14, full_width = T,
                bootstrap_options = c("striped", "hover"))
```

## Map of Predictions for Calgary Testing Set
To spatially investigate the distribution of these four predicted outcomes, the map here shows the four confusion matrix results associated with each fishnet grid cell in the testing set. We elected to only show the predicts for the testing set since the amount of observations was already at a large number and patterns are already discernible.

Note how there are few false positives throughout the city, indicating that there are few places for which the model did not accurately predict inundation. The true positives are concentrated around the area where we saw major hydrological features and the stream network. 
```{r message=FALSE, warning=FALSE}
test_predictions <- testProbs %>%
  mutate(TN = predClass==0 & obs==0,
         TP = predClass==1 & obs==1,
         FN =  predClass==0 & obs==1,
         FP = predClass==1 & obs==0)


test_predictions <- test_predictions %>%
  mutate(confResult=case_when(TN == TRUE ~ "True_Negative",
                              TP == TRUE ~ "True_Positive",
                              FN == TRUE ~ "False_Negative",
                              FP == TRUE ~ "False_Positive"))


#join with geometry for the map of prediction classes
cal_test_predictions_mapdata <- cbind(calgaryTest, test_predictions, by= "ID_FISHNET") %>% st_as_sf()


ggplot() + 
    geom_sf(data=cal_test_predictions_mapdata, aes(fill=confResult), colour=NA)+
  scale_fill_discrete()+
  mapTheme() +
  labs(title="Map of Prediction Classes on Calgary Test")

```


### ROC Curve
The Receiver Operating Characteristic (ROC) Curve for the model is shown below. This curve is a helpful goodness of fit indicator, while helping to visualize trade-offs between true positive and false positive metrics at each threshold from 0.01 to 1. A line going “over” the curve indicates a useful fit.
The area under the curve (AUC) here is about 0.96, indicating a useful fit. A reasonable AUC is between 0.5 and 1.

```{r message=FALSE, warning=FALSE}
ggplot(testProbs, aes(d = obs, m = pred)) + 
  geom_roc(n.cuts = 50, labels = FALSE) + 
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') + labs(title="ROC Curve", subtitle="AUC ~ 0.96") + plotTheme()
```
```{r message=FALSE, warning=FALSE}
auc_calgary <- auc(testProbs$obs, testProbs$pred)
print(auc_calgary)
```


## Cross-Validation
While the ROC indicates a relatively good fit, cross-validation is key to ensure that the model generalizes well across contexts. A k-fold cross validation (CV) is performed here.

```{r message=FALSE, warning=FALSE}
#convert inundation to factor
calgary_fishnet_final$inundation <-as.factor(as.character(calgary_fishnet_final$inundation))

ctrl <- trainControl(method = "cv", 
                     number = 100,
                     savePredictions = TRUE)

cvFit_inundationModel <- train(as.factor(inundation) ~ ., data = calgary_fishnet_final %>% 
                            as.data.frame() %>%
                            select(-ID_FISHNET, -geometry, -slope_max, -imperv_pixels_sum),
                          method="glm", family="binomial", trControl=ctrl)

cvFit_inundationModel
```

### Accuracy and Kappa
To evaluate the algorithm further, the accuracy an kappa metrics shed light on goodness of fit. **Accuracy** is the percentage of instances classified correctly out of the total. While this does not provide the nuance provided in the confusion matrix (showing the breakdown of accuracy across the four classes), it is useful in defending the model's success at a high level.

**Kappa** (also known as Cohen's Kappa) is normalized accuracy. This is useful here because we have an imbalance in the 0s and 1s for no inundation and inundation, respectively. While the imbalance is expected given that the previous flood extent showed that the majority of Calgary was not inundated, Kappa is useful in its comparison of observed accuracy to expected accuracy, taking into account random chance. It is still a relatively high value with a mean of over 68%. According to best practices in statistics, a kappa value of 0.68 falls within the range of “substantial agreement.”


```{r eval=FALSE, include=FALSE}
ggplot(as.data.frame(cvFit_inundationModel$resample), aes(Accuracy)) + 
  geom_histogram() +
  scale_x_continuous(limits = c(0, 1)) +
  labs(x="Accuracy",
       y="Count") +plotTheme()
```

```{r message=FALSE, warning=FALSE}
dplyr::select(cvFit_inundationModel$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(cvFit_inundationModel$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) +
    geom_histogram(bins=35, fill = "navy") +
    facet_wrap(~metric) +
    geom_vline(aes(xintercept = mean), colour = "red", linetype = 3, size = 1.5) +
    scale_x_continuous(limits = c(0, 1)) +
    labs(x="Goodness of Fit", y="Count", title="Accuracy and Kappa",
         subtitle = "Across-fold mean represented as dotted lines") +plotTheme()
```

## Mapping Calgary Predictions
The final predictions for inundation across Calgary are mapped for each fishnet grid cell. We elected to display the predictions on a probabilities scale to achieve a more continuous surface.

```{r message=FALSE, warning=FALSE}
allPredictions <- 
  predict(cvFit_inundationModel, calgary_fishnet_final, type="prob")[,2]
  
calgary_fishnet_final_preds <- calgary_fishnet_final %>%
  cbind(calgary_fishnet_final,allPredictions) %>%
  mutate(allPredictions = round(allPredictions * 100)) 
```

```{r message=FALSE, warning=FALSE}
ggplot() + 
    geom_sf(data=calgary_fishnet_final_preds, aes(fill=(allPredictions)), colour=NA) +
  scale_fill_viridis(name = "Probability")+
  mapTheme() +
  labs(title="Predictions for Inundation in Calgary")
```

This second map shows the predicted probabilities, but with the 2013 flood extent overlay. This helps to compare the predicted inundation outcomes with the actual data used to train the model in the first place.

```{r message=FALSE, warning=FALSE}
ggplot() + 
    geom_sf(data=calgary_fishnet_final_preds, aes(fill=(allPredictions)), colour=NA) +
     scale_fill_viridis(name = "Probability")+
  geom_sf(data = calgary_fishnet_final_preds %>%
          filter(inundation=="1"),
          aes(), color="transparent", fill="red", alpha=0.5)+
  mapTheme() +
  labs(title="Predictions for Inundation in Calgary", subtitle = "2013 flood extent in red overlay")
```

# Turning to Denver: How Does This Model Perform in Another City?
Now that we have confirmed the model’s high quality performance in Calgary, let’s see if is generalizable to other cities. Denver, Colorado has a similar topography, infrastructure, and population size to Calgary. We perform the same feature engineering for the selected features of the model and run the regression for the City of Denver. 

```{r Denver feature loading and engineering, message=FALSE, warning=FALSE, results='hide'}
den_boundary <- st_read("Data_Denver/county_boundary/county_boundary.shp") # need full shp package

# DENVER: Load Calgary fishnet generated in ArcGIS Pro - NEED IT AS A SHAPEFILE
den_fishnet <- st_read("Data_Denver/den_fishnet/den_fishnet.shp") %>% st_as_sf() %>% st_transform(st_crs(den_boundary))

den_fishnet$ID_FISHNET <-as.character(den_fishnet$ID_FISHNET)

# Denver: Elevation mean (calculated using zonal statistics (mean)).
den_elev <- st_read("Data_Denver/den_elev_mean.xls") %>%
  dplyr::select(FID, MEAN) %>%
  dplyr::rename(elevation_mean = MEAN) %>%
  dplyr::rename(ID_FISHNET = FID)

den_elev$ID_FISHNET <-as.character(den_elev$ID_FISHNET)

den_elev_grid <- full_join(den_elev, den_fishnet) %>% st_as_sf()
den_fishnet_new <- den_elev_grid

# DENVER: Streams - distance
den_streams_neardist <- st_read("Data_Denver/den_dist_streams2.xls")  %>%
  dplyr::select(-OBJECTID, -NEAR_FID) %>%
  dplyr::rename(ID_FISHNET = IN_FID) %>%
  dplyr::rename(dist_stream = NEAR_DIST)

den_streams_neardist$ID_FISHNET <-as.character(den_streams_neardist$ID_FISHNET)
den_streams_neardist_grid <- full_join(den_streams_neardist, den_fishnet_new) %>% st_as_sf()
den_fishnet_new <-  den_streams_neardist_grid

# DENVER: flow accumulation max per fishnet cell. Generated using zonal statistics (maximum).
den_flowacc <- st_read("Data_Denver/den_fac_max.xls") %>%
  dplyr::rename(flowacc_max = MAX) %>%
  dplyr::select(-COUNT, -AREA) %>%
  dplyr::rename(ID_FISHNET = FID)

den_flowacc$ID_FISHNET <-as.character(den_flowacc$ID_FISHNET)
den_flowacc_grid <- full_join(den_flowacc, den_fishnet_new) %>%st_as_sf()
den_fishnet_new <- den_flowacc_grid 

# DENVER: Parks distance - from open data calgary. Used arcgis pro to create a near distance table.
den_parks_neardist <- st_read("Data_Denver/den_dist_parks2.xls")  %>%
  dplyr::select(-OBJECTID, -NEAR_FID) %>%
  dplyr::rename(ID_FISHNET = IN_FID) %>%
  dplyr::rename(dist_parks = NEAR_DIST)

den_parks_neardist$ID_FISHNET <-as.character(den_parks_neardist$ID_FISHNET)
den_parks_neardist_grid <- full_join(den_parks_neardist, den_fishnet_new) %>% st_as_sf()
den_fishnet_new <-  den_parks_neardist_grid

# DENVER:  hydro shapefile import
den_hydro <- st_read("Data_Denver/den_water/den_water.shp") %>%
  st_transform(st_crs(den_fishnet_new)) %>% st_as_sf()

# DENVER: hydro features intersections
den_fishnet_new <- den_fishnet_new %>% mutate(n_hydro_int = lengths(st_intersects(den_fishnet_new, den_hydro)))

# DENVER: Distance to steep slopes per fishnet cell. Generated using near table, near distance.
den_steepslope_dist <- st_read("Data_Denver/den_dist_slope2.xls")  %>%
  dplyr::select(-OBJECTID, -NEAR_FID) %>%
  dplyr::rename(ID_FISHNET = IN_FID) %>%
  dplyr::rename(steepslope_dist = NEAR_DIST)

den_steepslope_dist$ID_FISHNET <-as.character(den_steepslope_dist$ID_FISHNET)
den_steepslope_dist_grid <- full_join(den_steepslope_dist, den_fishnet_new) %>%st_as_sf()
den_fishnet_new <- den_steepslope_dist_grid 

# Remove unnecessary columns
den_fishnet_new <- den_fishnet_new %>%
    dplyr::select(-Id, -OBJECTID) 

```

```{r denver select by centroids, message=FALSE, warning=FALSE, results='hide'}
den_selectCentroids <-
  st_centroid(den_fishnet_new)[den_boundary,] %>%
  st_drop_geometry() %>%
  left_join(dplyr::select(den_fishnet_new, ID_FISHNET)) %>%
  st_sf()

den_fishnet_final <- den_selectCentroids
```

```{r Denver predict, message=FALSE, warning=FALSE, results='hide'}
allPredictions_den <- 
  predict(inundationModel, den_fishnet_final, type="response")


den_fishnet_final_preds <- den_fishnet_final %>%
  cbind(den_fishnet_final, allPredictions_den) %>%
  mutate(allPredictions_den = round(allPredictions_den*100)) %>% st_as_sf()
```

```{r denver predictions map, message=FALSE, warning=FALSE}
ggplot() + 
    geom_sf(data=den_fishnet_final_preds, aes(fill=(allPredictions_den)), colour=NA) +
  scale_fill_viridis(name = "Probability")+
  mapTheme() +
  labs(title="Predictions for Inundation in Denver")

```

# Conclusion
The results of this model are crucial to informing planners and allowing a city to be prepared for flood disasters. Successfully leveraging the knowledge and insight gained can save lives, cities, homes, and money. There are many implementation challenges when it comes to appropriately executing emergency preparedness programs such as funding, political and community support, and collaboration across departments. However, we are confident that the story being told through these data visualizations can be an effective tool in convincing relevant decision-makers.




